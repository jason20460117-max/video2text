import streamlit as st
import time
import os
import glob
from openai import OpenAI

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from video_utils import extract_url, download_video_logic
from prompts import PROMPT_MAP, TEMP_MAP

# å°è¯•å¯¼å…¥æ ¸å¿ƒ AI åº“
try:
    import whisper
    import torch
    import zhconv
except ImportError:
    st.error("âš ï¸ æ£€æµ‹åˆ°ç¼ºå°‘å¿…è¦åº“ï¼è¯·è¿è¡Œ: pip install yt-dlp openai-whisper torch zhconv")
    st.stop()

# ==========================================
# 1. é¡µé¢åŸºç¡€é…ç½®ä¸ç²¾ç¾ CSS 
# ==========================================
st.set_page_config(
    page_title="DeepFlow v8.3 (Modular)",
    page_icon="ğŸŒŠ",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .stApp {background-color: #FFFFFF !important; color: #000000 !important;}
    section[data-testid="stSidebar"] {background-color: #F8F9FA !important; border-right: 1px solid #E9ECEF !important;}
    h1, h2, h3, h4, h5, h6, p, span, label, div { color: #212529 !important; line-height: 1.6 !important; }
    
    /* è¿›åº¦å—æ ·å¼ (Chunk Boxes) */
    .chunk-container { display: flex; flex-wrap: wrap; gap: 8px; margin-bottom: 15px; padding: 10px; background-color: #f8f9fa; border-radius: 8px; border: 1px solid #dee2e6; }
    .chunk-box { width: 32px; height: 32px; display: flex; align-items: center; justify-content: center; border-radius: 6px; font-weight: bold; font-size: 14px; transition: all 0.3s ease; }
    .chunk-pending { background-color: #ffffff; border: 2px solid #dee2e6; color: #adb5bd; }
    .chunk-active { background-color: #e7f1ff; border: 2px solid #0d6efd; color: #0d6efd; box-shadow: 0 0 8px rgba(13, 110, 253, 0.3); }
    .chunk-done { background-color: #198754; border: 2px solid #198754; color: #ffffff; }

    button[kind="primary"] {background-color: #198754 !important; border-color: #198754 !important; color: #FFFFFF !important;}
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ ¸å¿ƒè¾…åŠ©å‡½æ•° 
# ==========================================
def render_chunk_visualizer(total, current_index, container):
    html = '<div class="chunk-container">'
    for i in range(total):
        state = "chunk-done" if i < current_index else ("chunk-active" if i == current_index else "chunk-pending")
        html += f'<div class="chunk-box {state}">{i+1}</div>'
    html += '</div>'
    container.markdown(html, unsafe_allow_html=True)

def get_device_status():
    if torch.cuda.is_available():
        return "cuda", f"âœ… GPU ({torch.cuda.get_device_name(0)})"
    return "cpu", "âš ï¸ CPU (Slow)"

@st.cache_resource
def load_whisper_model(model_size="base"):
    device, _ = get_device_status()
    return whisper.load_model(model_size, device=device)

def transcribe_logic(file_path, model_size="base"):
    model = load_whisper_model(model_size)
    result = model.transcribe(file_path, language="zh", initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯å†…å®¹ã€‚")
    return zhconv.convert(result["text"], 'zh-cn')

def smart_split_text(text, max_chars=1200):
    chunks, current = [], ""
    for p in text.split('\n'):
        if len(current) + len(p) < max_chars: current += p + "\n"
        else:
            if current: chunks.append(current)
            current = p + "\n"
    if current: chunks.append(current)
    return chunks

# ==========================================
# 3. ä¾§è¾¹æ ä¸çŠ¶æ€åˆå§‹åŒ– 
# ==========================================
with st.sidebar:
    st.markdown("## ğŸŒŠ DeepFlow v8.3")
    _, device_msg = get_device_status()
    st.info(device_msg)
    
    app_mode = st.radio("åŠŸèƒ½å¯¼èˆª", ["ğŸ“ æ–‡æœ¬æ™ºèƒ½æ¶¦è‰²", "ğŸ¬ è§†é¢‘ä¸‹è½½ä¸è½¬å½•"])
    st.divider()

    if app_mode == "ğŸ“ æ–‡æœ¬æ™ºèƒ½æ¶¦è‰²":
        api_key = st.text_input("API Key", type="password")
        base_url = st.text_input("Base URL", value="https://api.deepseek.com")
        model_name = st.selectbox("æ¨¡å‹", ["deepseek-chat", "gpt-4o-mini"])
        
        selected_preset = st.selectbox("ä»»åŠ¡é¢„è®¾", list(PROMPT_MAP.keys()))
        sys_prompt = st.text_area("ç³»ç»Ÿæç¤ºè¯", value=PROMPT_MAP[selected_preset], height=150)
        temp = st.slider("åˆ›æ„æ¸©åº¦", 0.0, 1.5, value=TEMP_MAP[selected_preset])
        chunk_size = st.number_input("åˆ†æ®µå­—ç¬¦æ•°", 500, 4000, 1500)
    else:
        w_size = st.selectbox("Whisperæ¨¡å‹", ["tiny", "base", "small", "medium"], index=1)

# ==========================================
# 4. ä¸»ç•Œé¢é€»è¾‘ 
# ==========================================
if app_mode == "ğŸ“ æ–‡æœ¬æ™ºèƒ½æ¶¦è‰²":
    st.subheader("ğŸ“„ æ–‡æœ¬å¤„ç†")
    user_input = st.text_area("è¯·è¾“å…¥åŸå§‹æ–‡æœ¬", height=300)
    
    if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary", use_container_width=True):
        if not api_key or not user_input:
            st.warning("è¯·æ£€æŸ¥ API Key å’Œè¾“å…¥å†…å®¹")
        else:
            client = OpenAI(api_key=api_key, base_url=base_url)
            chunks = smart_split_text(user_input, max_chars=chunk_size)
            full_res, vis_place = "", st.empty()
            
            for idx, chunk in enumerate(chunks):
                render_chunk_visualizer(len(chunks), idx, vis_place)
                st.caption(f"æ­£åœ¨å¤„ç†ç¬¬ {idx+1}/{len(chunks)} æ®µ...")
                try:
                    resp = client.chat.completions.create(
                        model=model_name,
                        messages=[{"role":"system","content":sys_prompt},{"role":"user","content":chunk}],
                        temperature=temp, stream=True
                    )
                    full_res += st.write_stream(resp) + "\n\n"
                except Exception as e:
                    st.error(f"å‡ºé”™: {e}"); break
            
            render_chunk_visualizer(len(chunks), len(chunks), vis_place)
            st.success("å…¨éƒ¨å¤„ç†å®Œæˆï¼")
            st.text_area("åˆå¹¶ç»“æœ", full_res, height=300)

elif app_mode == "ğŸ¬ è§†é¢‘ä¸‹è½½ä¸è½¬å½•":
    st.subheader("ğŸ”— è§†é¢‘é“¾æ¥è½¬å½•")
    url_input = st.text_input("ç²˜è´´ Bç«™/YouTube é“¾æ¥")
    
    if st.button("â¬‡ï¸ ä¸‹è½½å¹¶è½¬å½•", type="primary"):
        real_url = extract_url(url_input)
        if real_url:
            with st.status("å¤„ç†ä¸­...") as s:
                res = download_video_logic(real_url, mode="audio")
                if res["status"] == "success":
                    s.write("ä¸‹è½½æˆåŠŸï¼Œå¼€å§‹è½¬å½•...")
                    text = transcribe_logic(res["file_path"], w_size)
                    st.session_state.last_text = text
                    s.update(label="âœ… å®Œæˆ", state="complete")
                    st.text_area("è½¬å½•ç»“æœ", text, height=300)
                else: st.error(res["msg"])
        else: st.error("æ— æ•ˆé“¾æ¥")